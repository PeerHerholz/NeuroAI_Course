{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d207f0be-cbd8-4b93-a823-61af51421e2a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# W1D1 Tutorial1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57acdfc5-c864-40a0-b648-be385d5c3eb5",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "#!pip install numpy Pillow matplotlib torch torchvision transformers gradio sentencepiece protobuf\n",
    "#!pip install git+https://github.com/Belval/TextRecognitionDataGenerator#egg=trdg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40270953",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "\n",
    "# Standard Libraries for file and operating system operations, security, and web requests\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import logging\n",
    "import io\n",
    "\n",
    "# Core python data science and image processing libraries\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep Learning and model specific libraries\n",
    "import torch\n",
    "import transformers\n",
    "from torchvision import transforms\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import google.protobuf\n",
    "\n",
    "# Utility and interface libraries\n",
    "import gradio as gr\n",
    "from IPython.display import IFrame\n",
    "import trdg\n",
    "from trdg.generators import GeneratorFromStrings\n",
    "import sentencepiece\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Figure settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa95f5",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\") # update this to match your course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plotting functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf34b9a-1dd5-458a-b390-0fa12609d532",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def display_image(image_path):\n",
    "    \"\"\"Display an image from a given file path.\n",
    "\n",
    "    Inputs:\n",
    "    - image_path (str): The path to the image file.\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.show()\n",
    "\n",
    "def display_transformed_images(image, transformations):\n",
    "    \"\"\"\n",
    "    Apply a list of transformations to an image and display them.\n",
    "\n",
    "    Inputs:\n",
    "    - image (Tensor): The input image as a tensor.\n",
    "    - transformations (list): A list of torchvision transformations to apply.\n",
    "    \"\"\"\n",
    "    # Convert tensor image to PIL Image for display\n",
    "    pil_image = transforms.ToPILImage()(image)\n",
    "\n",
    "    fig, axs = plt.subplots(len(transformations) + 1, 1, figsize=(5, 15))\n",
    "    axs[0].imshow(pil_image)\n",
    "    axs[0].set_title('Original')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, transform in enumerate(transformations):\n",
    "        # Apply transformation if it's not the placeholder\n",
    "        if transform != \"Custom ElasticTransform Placeholder\":\n",
    "            transformed_image = transform(image)\n",
    "            # Convert transformed tensor image to PIL Image for display\n",
    "            display_image = transforms.ToPILImage()(transformed_image)\n",
    "            axs[i+1].imshow(display_image)\n",
    "            axs[i+1].set_title(transform.__class__.__name__)\n",
    "            axs[i+1].axis('off')\n",
    "        else:\n",
    "            axs[i+1].text(0.5, 0.5, 'ElasticTransform Placeholder', ha='center')\n",
    "            axs[i+1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_original_and_transformed_images(original_tensor, transformed_tensor):\n",
    "    \"\"\"\n",
    "    Display the original and transformed images side by side.\n",
    "\n",
    "    Inputs:\n",
    "    - original_tensor (Tensor): The original image as a tensor.\n",
    "    - transformed_tensor (Tensor): The transformed image as a tensor.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Display original image\n",
    "    original_image = original_tensor.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    axs[0].imshow(original_image)\n",
    "    axs[0].set_title('Original')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Display transformed image\n",
    "    transformed_image = transformed_tensor.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    axs[1].imshow(transformed_image)\n",
    "    axs[1].set_title('Transformed')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def display_generated_images(generator):\n",
    "    \"\"\"\n",
    "    Display images generated from strings.\n",
    "\n",
    "    Inputs:\n",
    "    - generator (GeneratorFromStrings): A generator that produces images from strings.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, (text_img, lbl) in enumerate(generator, 1):\n",
    "        ax = plt.subplot(1, len(generator.strings) * generator.count // len(generator.strings), i)\n",
    "        plt.imshow(text_img)\n",
    "        plt.title(f\"Example {i}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dbda69",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "def download_image(fname, url, expected_md5):\n",
    "    \"\"\"\n",
    "    Downloads an image file from the given URL and saves it locally.\n",
    "\n",
    "    Inputs:\n",
    "    - fname (str): The local filename/path to save the downloaded image.\n",
    "    - url (str): The URL from which to download the image.\n",
    "    - expected_md5 (str): The expected MD5 checksum to verify the integrity of the downloaded data.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url) # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content) # Write the downloaded content to a file\n",
    "\n",
    "# Variables for file and download URL\n",
    "fnames = [\"img_1235.jpg\", \"image_augmentation.png\"]  # The names of the files to be downloaded\n",
    "urls = [\"https://osf.io/kv5bx/download\", \"https://osf.io/fqwsr/download\"]  # URLs from where the files will be downloaded\n",
    "expected_md5s = [\"920ae567f707bfee0be29dc854f804ed\", \"f4f1ebee1470a7e2d7662eec1d193ba2\"] # MD5 hashes for verifying files integrity\n",
    "\n",
    "for fname, url, expected_md5 in zip(fnames, urls, expected_md5s):\n",
    "    download_image(fname, url, expected_md5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a34ac-fa41-4e90-ab45-82c14384a83e",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "def download_file(fname, url, expected_md5):\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL and saves it locally.\n",
    "\n",
    "    Inputs:\n",
    "    - fname (str): The local filename/path to save the downloaded file.\n",
    "    - url (str): The URL from which to download the file.\n",
    "    - expected_md5 (str): The expected MD5 checksum to verify the integrity of the downloaded data.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url)  # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content)  # Write the downloaded content to a file\n",
    "                print(f\"{fname} has been downloaded successfully.\")\n",
    "\n",
    "# Variables for the font file and download URL\n",
    "fname = \"Dancing_Script.zip\"\n",
    "url = \"https://osf.io/32yed/download\"\n",
    "expected_md5 = \"d59bd3201b58a37d0d3b4cd0b0ec7400\"\n",
    "\n",
    "# Download the font file\n",
    "download_file(fname, url, expected_md5)\n",
    "\n",
    "def extract_zip(zip_fname):\n",
    "    \"\"\"\n",
    "    Extracts a ZIP file to the current directory.\n",
    "\n",
    "    Inputs:\n",
    "    - zip_fname (str): The filename/path of the ZIP file to be extracted.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_fname, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "        print(f\"Extracted {zip_fname} successfully.\")\n",
    "\n",
    "# Extract the downloaded ZIP file\n",
    "extract_zip(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0210829-c944-4f16-b8e3-bd4ece058887",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Before diving into the interactive demo, we'll explore why handwriting recognition remains a pivotal challenge in AI,\n",
    "#underlining its relevance in today's technology-driven world.\n",
    "##This activity demonstrates TrOCR's capabilities in recognizing diverse handwriting styles, providing insight into\n",
    "#how modern AI systems interpret human writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00326ca-807d-460f-adf4-767e94bc0ccc",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained TrOCR model and processor\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-handwritten\")\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-handwritten\")\n",
    "\n",
    "# Define the function to recognize text from an image\n",
    "def recognize_text(image):\n",
    "    \"\"\"\n",
    "    This function takes an image as input and uses a pre-trained language model to generate text from the image.\n",
    "\n",
    "    Inputs:\n",
    "    - image (PIL Image or Tensor): The input image containing text to be recognized.\n",
    "\n",
    "    Outputs:\n",
    "    - text (str): The recognized text extracted from the input image.\n",
    "    \"\"\"\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return text\n",
    "\n",
    "# Create a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=recognize_text,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=gr.Textbox(),\n",
    "    title=\"Interactive demo: TrOCR\",\n",
    "    description=\"Demo for Microsoft’s TrOCR, an encoder-decoder model for OCR on single-text line images.\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc120906-e935-4604-a9bb-06eb611cfd84",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Prior to dissecting the TrOCR model, we discuss the significance of understanding a model's internal\n",
    "# architecture and how it contributes to its ability to recognize handwriting.\n",
    "## This segment prepares participants to explore the encoder-decoder architecture of TrOCR, emphasizing the\n",
    "# model's design choices and their impact on performance and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b10a2-a46c-4fe0-ad47-a059b110209f",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-handwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cd9c9-95fb-4541-9ece-78620745ee1f",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Inspect the encoder of the model\n",
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69695c6c-0ba3-426d-8af1-ab7e845dba14",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Inspect the decoder of the model\n",
    "model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62d46c-1985-4d8f-8cb9-4e02815e5e0e",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Introducing the concept of transfer learning, we discuss how leveraging pre-trained models\n",
    "# like RoBERTa can vastly reduce the need for extensive new data, by distilling knowledge from vast text corpora.\n",
    "\n",
    "## This activity aims to illustrate the efficiency of transfer learning in adapting to new tasks,\n",
    "# highlighting the scale of data involved and the computational efficiency it brings to modern AI solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df898317-31e6-4b28-9175-c1206b255a98",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Function to count the parameters of the model\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    This function calculates the total number of parameters in a given PyTorch model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The PyTorch model for which parameters are to be counted.\n",
    "\n",
    "    Outputs:\n",
    "    - num_parameters (int): The total number of parameters in the specified model.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Count parameters in the encoder\n",
    "encoder_params = count_parameters(model.encoder)\n",
    "\n",
    "# Count parameters in the decoder\n",
    "decoder_params = count_parameters(model.decoder)\n",
    "\n",
    "encoder_params, decoder_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ada4c5-ea4f-4feb-917d-1f9b43b11865",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def calculate_writing_time(total_words, words_per_day, days_per_week, weeks_per_year, average_human_lifespan):\n",
    "    \"\"\"\n",
    "    Calculate the time required to write a given number of words in lifetimes.\n",
    "\n",
    "    Inputs:\n",
    "    - total_words (int): total number of words to be written.\n",
    "    - words_per_day (int): number of words written per day.\n",
    "    - days_per_week (int): number of days dedicated to writing per week.\n",
    "    - weeks_per_year (int): number of weeks dedicated to writing per year.\n",
    "    - average_human_lifespan (int): average lifespan of a human in years.\n",
    "\n",
    "    Outpus:\n",
    "    - time_to_write_lifetimes (float): time to write the given words in lifetimes.\n",
    "    \"\"\"\n",
    "\n",
    "    #################################################\n",
    "    ## TODO for students: fill in the missing variables ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "    #################################################\n",
    "\n",
    "    words_per_year = words_per_day * days_per_week * weeks_per_year\n",
    "\n",
    "    # Calculate the time to write in years\n",
    "    time_to_write_years = total_words / ...\n",
    "\n",
    "    # Calculate the time to write in lifetimes\n",
    "    time_to_write_lifetimes = time_to_write_years / average_human_lifespan\n",
    "\n",
    "    return time_to_write_lifetimes\n",
    "\n",
    "# Example values\n",
    "total_words = 5e9\n",
    "words_per_day = 1500\n",
    "days_per_week = 6\n",
    "weeks_per_year = 50\n",
    "average_human_lifespan = 80\n",
    "\n",
    "# Uncomment the code below to test your function\n",
    "\n",
    "# Test the function\n",
    "#time_to_write_lifetimes_roberta = calculate_writing_time(\n",
    "    #total_words,\n",
    "    #words_per_day,\n",
    "    #days_per_week,\n",
    "    #weeks_per_year,\n",
    "    #average_human_lifespan\n",
    "#)\n",
    "\n",
    "# Print the result\n",
    "#print(f\"Time to write {total_words} words in lifetimes: {time_to_write_lifetimes_roberta} lifetimes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6127df-6dde-445f-8b49-3df6ba9064fb",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/course-content-template/tree/main/tutorials/W1D1_Generalization/solutions/W1D1_Tutorial1_Solution_f50cc7ce.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19006eb-4600-4aa4-9c99-b85b5e4a87c5",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#Exploring Llama 2\n",
    "total_tokens_llama2 = 2e12\n",
    "total_words_llama2 = 2e12 / 1.5 #assuming 1.5 words per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785a098-d6ff-48a8-8b57-3e0e9e436ffe",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Time to generate text\n",
    "time_to_write_lifetimes_llama = calculate_writing_time(total_words_llama2, words_per_day, days_per_week, weeks_per_year, average_human_lifespan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad1c92a-13e9-4109-b7c7-c961414fb9f7",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Before the coding exercise, we introduce the concept of data augmentation as a critical technique\n",
    "# or improving model generalization through enhanced data diversity.\n",
    "\n",
    "## Participants will learn how augmenting data can simulate real-world variability, teaching AI\n",
    "# systems to maintain accuracy even when presented with novel or altered inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78127c97-955e-46e0-869d-7f7285449f70",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "image_path = 'img_1235.jpg'\n",
    "display_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0a320-46f3-4f84-8c51-c8cd07cea776",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Convert PIL Image to Tensor\n",
    "image = Image.open(image_path)\n",
    "image = transforms.ToTensor()(image)\n",
    "\n",
    "# Define each transformation separately\n",
    "# RandomAffine: applies rotations, translations, scaling. Here, rotates by up to ±15 degrees,\n",
    "affine = transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1))\n",
    "\n",
    "# ElasticTransform: applies elastic distortions to the image. The 'alpha' parameter controls\n",
    "# the intensity of the distortion.\n",
    "elastic = transforms.ElasticTransform(alpha=50.0)\n",
    "\n",
    "# RandomPerspective: applies random perspective transformations with a specified distortion scale.\n",
    "perspective = transforms.RandomPerspective(distortion_scale=0.2, p=1.0)\n",
    "\n",
    "# RandomErasing: randomly erases a rectangle area in the image.\n",
    "erasing = transforms.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random', inplace=False)\n",
    "\n",
    "# GaussianBlur: applies gaussian blur with specified kernel size and sigma range.\n",
    "gaussian_blur = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.4, 5))\n",
    "\n",
    "# A list of all transformations for iteration\n",
    "transformations = [affine, elastic, perspective, erasing, gaussian_blur]\n",
    "\n",
    "# Display\n",
    "display_transformed_images(image, transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c000dc-2c1e-41e6-a988-ab75819b0b39",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Path to the image\n",
    "image_path = 'image_augmentation.png'\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert PIL Image to Tensor\n",
    "image_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "# Define transformations here\n",
    "affine = transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1))\n",
    "elastic = transforms.ElasticTransform(alpha=90.0)\n",
    "perspective = transforms.RandomPerspective(distortion_scale=0.2, p=1.0)\n",
    "erasing = transforms.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random', inplace=False)\n",
    "gaussian_blur = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "\n",
    "# Combine all the transformations\n",
    "all_transforms = transforms.Compose([\n",
    "    affine,\n",
    "    elastic,\n",
    "    perspective,\n",
    "    erasing,\n",
    "    gaussian_blur\n",
    "])\n",
    "\n",
    "# Apply combined transformation\n",
    "augmented_image_tensor = all_transforms(image_tensor)\n",
    "\n",
    "display_original_and_transformed_images(image_tensor, augmented_image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdee309-1b4e-45af-b9ab-cf12911f807c",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## We preface this activity by discussing the limitations of real-world data and the role of\n",
    "# synthetic data in overcoming these barriers, particularly in domains like handwriting recognition where variability is vast.\n",
    "## This segment sets the stage for participants to engage with synthetic data generation techniques,\n",
    "# exploring how they can extend the model's ability to generalize beyond its original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1a69c-005a-41c1-8014-8fe9cb4c8ea7",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define your strings\n",
    "strings = ['Hello', 'This is Patrick', 'From NMA']\n",
    "\n",
    "# Specify font path\n",
    "font_path = \"DancingScript-VariableFont_wght.ttf\"  # Ensure this path is correct\n",
    "\n",
    "# Create a generator with the specified parameters\n",
    "generator = GeneratorFromStrings(\n",
    "    strings=strings,\n",
    "    fonts=[font_path],\n",
    "    space_width=2,\n",
    "    skewing_angle=8,\n",
    "    count=3\n",
    ")\n",
    "\n",
    "# Define the desired size\n",
    "desired_size = (500, 300)  # Width, Height in pixels\n",
    "\n",
    "# Function to resize images\n",
    "def resize_image(image, new_size):\n",
    "    return image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "for img, lbl in generator:\n",
    "    # Resize the image before showing it\n",
    "    img = resize_image(img, desired_size)\n",
    "    img.show()\n",
    "\n",
    "# Call the function with the generator\n",
    "display_generated_images(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca19de-44b0-492d-af5b-8fa78fc5e51e",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.calligrapher.ai/\", width=800, height=600)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D1_Tutorial1",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}